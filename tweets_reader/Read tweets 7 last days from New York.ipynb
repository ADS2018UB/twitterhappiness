{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pymongo\n",
    "import tweepy\n",
    "\n",
    "with open('consumer_key.txt', 'r') as f:\n",
    "    consumer_key =  f.read().strip()\n",
    "f.closed\n",
    "\n",
    "with open('consumer_secret.txt', 'r') as f:\n",
    "    consumer_secret = f.read().strip()\n",
    "f.closed\n",
    "\n",
    "with open('access_key.txt', 'r') as f:\n",
    "    access_key = f.read().strip()\n",
    "f.closed\n",
    "\n",
    "with open('access_secret.txt', 'r') as f:\n",
    "     access_secret = f.read().strip()\n",
    "f.closed\n",
    "\n",
    "#Authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a Standard API you can only access tweets from the last 7 days, so there is no need for a filter. However, we should store those tweets if we want to create historical data, otherwise we won't have access to them after 7 days. We can also restrict the language of the tweets, which is a good idea since our machine learning model will only work on English tweets. We can restrict the geolocation using geo_search or using coordinates inside api.search within the parameter geocode. The tweets are chosen randomly. However there are some limitiations. Only 180 queries are allowed every 15min. In 180 queries we can get approximately seven thousand tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So many great options.\n",
      "Like a candy store.\n",
      "For the modern #legal industry.\n",
      "\n",
      "https://t.co/KwmAj6TY0H #hipcounsel‚Ä¶ https://t.co/8nM5ZzA9dc | Manhattan\n",
      "My mom is really the Starbucks üîå \n",
      "I never pay for anything when I‚Äôm with her | Queens\n",
      "@Iamyoz @wheatus Woohoo! See you then | Queens\n",
      "I told my friends that I am a transitioning adult. I wake up every day at 5:20am and get to work early, but I do re‚Ä¶ https://t.co/kRvUrvlFC3 | Bronx\n"
     ]
    }
   ],
   "source": [
    "places = api.geo_search(query=\"New York\", granularity=\"city\")\n",
    "place_id = places[0].id\n",
    "\n",
    "tweets = api.search(q=\"place:%s\" % place_id, lang='en', count=10)\n",
    "for tweet in tweets:\n",
    "    print(tweet.text + \" | \" + tweet.place.name if tweet.place else \"Undefined place\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to get a high amount of tweets we can loop over the maximum number of queries. The following cell keeps storing tweets to the mongo db database and freezes 15 minutes every time we reach the maximum number of queries. We can break the loop whenever we feel we have enough tweets by changing the parameter K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "max_queries = 180\n",
    "count = 0\n",
    "K = 50\n",
    "\n",
    "while True:\n",
    "    for i in range(max_queries):\n",
    "        tweets = api.search(q=\"place:%s\" % place_id, lang='en', count=1000)\n",
    "            for tweet in tweets:\n",
    "                #save to mongodb\n",
    "                pass\n",
    "    count +=1\n",
    "    if count > K: break\n",
    "    time.sleep(910)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also stream tweets from New York"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** esjacobs ** : @WalshFreedom you yesterday: ‚Äúvote for republicans!‚Äù\n",
      "** Silvanita2812 ** : ‚ÄúTodos me dicen la negra, llorona...‚Äù https://t.co/FMJJNpT7xz\n",
      "** Brittnaeee ** : The city is mineüìç https://t.co/JxknCDfAfb\n",
      "** Aoiferocksitout ** : @brooklynburning Saaaaame!! ‚ù§Ô∏è‚ù§Ô∏è\n",
      "** spanishzac ** : PEOPLE OVER 75 SHOULD NOT BE ALLOWED TO VOTE!!!!! They will not be here to see the consequences of their actions!!!!\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from tweepy import Stream,StreamListener\n",
    "\n",
    "class listener(StreamListener):\n",
    "    def __init__(self):\n",
    "        super(StreamListener, self).__init__()\n",
    "        self.num_tweets = 0\n",
    "        \n",
    "    def on_data(self, status):\n",
    "        if self.num_tweets < 5:\n",
    "            json_data=json.loads(status)\n",
    "            print (str('** '+json_data[\"user\"][\"screen_name\"])+' ** : ' + json_data[\"text\"])\n",
    "            self.num_tweets += 1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def on_error(self, status):\n",
    "        print (status)\n",
    "        \n",
    "# Catch all tweets in New York area and print them\n",
    "twitterStream = Stream(auth, listener()) \n",
    "twitterStream.filter(locations=[-74.1781,40.5609,-73.7468,40.8866])\n",
    "print (\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
